{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Dataset : PyMapD- XGBoost - MapD\n",
    "# Response Variable: Evasion_24M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymapd\n",
    "import pygdf \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "le= LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up MapD connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection(mapd://mapd:***@http://localhost:9090/mapd?protocol=http)\n"
     ]
    }
   ],
   "source": [
    "dbname    = 'mapd'\n",
    "username  = 'mapd'\n",
    "password  = 'HyperInteractive'\n",
    "hostname  = 'localhost'\n",
    "port      = 9090\n",
    "\n",
    "con = pymapd.connect(user=username,password=password,dbname=dbname,host=hostname,port=port,protocol='http')\n",
    "print(con)\n",
    "c   = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from MapD to PyGDF\n",
    "\n",
    "1. Create Table evasion_v2 in mapdql\n",
    "2. load data into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 49\n"
     ]
    }
   ],
   "source": [
    "table= 'evasion_v2'\n",
    "response= 'EVASION_FLAG_24M' #aka target variable or label\n",
    "\n",
    "date_column = '''REFERENCE_DATE'''\n",
    "\n",
    "columns= '''PRIVATE_CUSTOMER ,TRAIN_TEST ,NUMBER_OF_CAMPAIGNS_RECEIVED ,MARKETING_PERMISSION ,TELEPHONE_AND_MAIL_PERMISSION ,DURATION_OF_OWNERSHIP ,NUMBER_OF_CARS_OWNED_BEFORE ,CAR_AGE ,CAR_BOUGHT_AT_VW_DEALER ,CAR_MODEL ,CAR_PRICE ,CO2_EMISSIONS ,PRODUCTION_YEAR ,EXTENDED_WARRANTY ,SERVICE_AND_MAINTEN_PACKAGE ,WARRANTY_LEFT ,ECONOMY_PARTS_12M ,MAINTENANCE_COSTS ,MAINTENANCE_COSTS_12M ,NUM_MAINTENANCE ,NUM_MAINTENANCE_12M , NUM_REPAIRS ,NUM_REPAIRS_12M ,NUM_WARRANTY ,REPAIR_COSTS ,REPAIR_COSTS_12M ,SERVICE_COSTS ,SERVICE_COSTS_12M ,TOTAL_COSTS ,WARRANTY_COSTS ,WARRANTY_COSTS_12M ,AVG_DURATION ,MILEAGE ,NEXT_MOT ,NUM_WORKSHOP_VISITS ,NUM_WORKSHOP_VISITS_12M ,SHARE_REPAIR_CASES ,SHARE_REPAIR_CASES_12M ,VIN_HASHED ,CUSTOMER_ID_HASHED ,MODEL_CODE,ENGINE_POWER ,ENGINE_POWER_KW_0  ,ENGINE_POWER_KW_1 ,ENGINE_POWER_COL1_0 ,HORSE_POWER  ,HORSE_POWER_0  ,HORSE_POWER_1'''\n",
    "columns_str= '''CAR_MODEL,PRODUCTION_YEAR'''\n",
    "\n",
    "index_columns = '''CUSTOMER_ID_HASHED,VIN_HASHED'''\n",
    "\n",
    "print('Number of Columns: %d'%(len((columns+','+response).split(','))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DF_Train and DF_Test\n",
    "\n",
    "* df_train is the training set\n",
    "* df_test is the test ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Training set: 1424232\n",
      "Number of rows in Test set: 357019\n"
     ]
    }
   ],
   "source": [
    "query_train = '''Select {},{} from {} Where train_test='train' '''.format(response,columns,table)\n",
    "query_test = '''Select {},rowid AS mapid,{} from {} Where train_test='test' '''.format(response,columns,table)\n",
    "\n",
    "query_index = '''Select {} from {} Where train_test='train' '''.format(index_columns,table)\n",
    "\n",
    "# implicit tdf to pygdf\n",
    "df_train = con.select_ipc_gpu(query_train,device_id=0)\n",
    "df_test  = con.select_ipc_gpu(query_test,device_id=0)\n",
    "\n",
    "index_df = con.select_ipc_gpu(query_index)\n",
    "index_df = index_df.to_pandas()\n",
    "#print for row_size check:\n",
    "print('Number of rows in Training set: %d'%(len(df_train)))\n",
    "print('Number of rows in Test set: %d'%(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unwanted columns/ Remove rowid \n",
    "\n",
    "\n",
    "**Remove these columns:**\n",
    "- reference_date (already removed)\n",
    "- private_customer\n",
    "- evasion flags\n",
    "- train_test\n",
    "- customer_id_hashed\n",
    "- vin_hashed\n",
    "- engine_power\n",
    "- engine power_kW_0\n",
    "- horse power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns that contain no additional information.\n",
    "rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','VIN_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "#rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "\n",
    "#vin_hash_series = df_train['VIN_HASHED'].to_pandas()\n",
    "\n",
    "for col in rm_cols:\n",
    "    df_train.drop_column(col)\n",
    "    df_test.drop_column(col)\n",
    "    #df_all.drop_column(col)\n",
    "    \n",
    "df_pred=pygdf.DataFrame()\n",
    "df_pred.add_column('mapid', df_test['mapid'])\n",
    "df_test.drop_column('mapid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical cols : Labels Encoding\n",
    "#### Comments:\n",
    "\n",
    "<span style=\"color:green\"> **Replacing categorical rows with no value with -1. The categories are then label encoded (0,1,2,3...)** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR_MODEL\n",
      "PRODUCTION_YEAR\n"
     ]
    }
   ],
   "source": [
    "for col in columns_str.split(','):\n",
    "    ctrain= df_train[col].fillna(-1).to_pandas()\n",
    "    ctest= df_test[col].fillna(-1).to_pandas()\n",
    "    fit= le.fit(ctrain.astype(str))\n",
    "    df_train[col] = fit.transform(ctrain.astype(str))\n",
    "    print(col)\n",
    "    df_test[col] = fit.transform(ctest.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CAR_PRICE', 'NUMBER_OF_CARS_OWNED_BEFORE', 'PRODUCTION_YEAR', 'SERVICE_COSTS', 'WARRANTY_COSTS', 'ECONOMY_PARTS_12M', 'NUM_WARRANTY', 'TOTAL_COSTS', 'TELEPHONE_AND_MAIL_PERMISSION', 'CAR_BOUGHT_AT_VW_DEALER', 'SHARE_REPAIR_CASES', 'AVG_DURATION', 'SHARE_REPAIR_CASES_12M', 'REPAIR_COSTS_12M', 'CAR_AGE', 'DURATION_OF_OWNERSHIP', 'NUM_WORKSHOP_VISITS', 'CO2_EMISSIONS', 'NUMBER_OF_CAMPAIGNS_RECEIVED', 'NUM_REPAIRS', 'SERVICE_AND_MAINTEN_PACKAGE', 'NUM_REPAIRS_12M', 'NUM_MAINTENANCE', 'WARRANTY_COSTS_12M', 'NUM_MAINTENANCE_12M', 'WARRANTY_LEFT', 'REPAIR_COSTS', 'ENGINE_POWER_KW_1', 'SERVICE_COSTS_12M', 'MARKETING_PERMISSION', 'MAINTENANCE_COSTS_12M', 'MAINTENANCE_COSTS', 'ENGINE_POWER_COL1_0', 'NUM_WORKSHOP_VISITS_12M', 'NEXT_MOT', 'MILEAGE', 'CAR_MODEL', 'EXTENDED_WARRANTY'}\n"
     ]
    }
   ],
   "source": [
    "#assign the column names to columns list\n",
    "columns = set(df_train.columns)\n",
    "\n",
    "#remove the response column from columns and assign remaining columns to features list.\n",
    "features = columns - set([response])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill empty values with np.nan\n",
    "\n",
    "* empty values are filled with NaN as opposed to -999 so that the partial dependence plots are not affected by -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    df_train[col] = df_train[col].fillna(np.nan)   # Treat missing values\n",
    "    df_test[col] = df_test[col].fillna(np.nan)\n",
    "    #since mapd matrix is not being used, consistent datatype did not have to be used for the xgboost DMatrix.\n",
    "    #df_train[col] = df_train[col].astype(np.float32) # Make consistent datatype\n",
    "    #df_test[col] = df_test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train and test target columns in to separate dataframes\n",
    "\n",
    "* df_train_y = the label (target) values for the training set\n",
    "* df_test_y = the label (target) values for the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the _y variables refer to the response (target) values of the dataframe\n",
    "df_train_y= df_train[response]\n",
    "df_test_y= df_test[response]\n",
    "\n",
    "del df_train[response]\n",
    "del df_test[response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the pygdf dataframe to pandas dataframe\n",
    "\n",
    "* Unfortunately, data must be copied back to cpu\n",
    "* Further work: Having XGBoost be able to work directly on the gpu with pygdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_pd = df_train.to_pandas()\n",
    "df_test_pd  = df_test.to_pandas()\n",
    "\n",
    "df_train_y_pd = df_train_y.to_pandas()\n",
    "df_test_y_pd = df_test_y.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: XGBoost\n",
    "\n",
    "* For more information about XGBoost, parameter tuning etc, refer to churn_24M-XGBoost_Tuning notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters\n",
    "\n",
    "### Comments:\n",
    "<span style=\"color:red\"> **Parameters are optimized in XGBoost Tuning notebook** </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params= {}\n",
    "params['objective']= 'binary:logistic'\n",
    "params['eval_metric']= 'auc'\n",
    "params['max_depth']= 7\n",
    "params['learning_rate']= 0.05\n",
    "#params['n_estimators']= 20 #if using XGBClassifier scikit wrapper\n",
    "params['num_rounds']= 100 #if using regular xgb\n",
    "params['min_child_weight'] = 1\n",
    "params['silent']= 0\n",
    "#params['nthread'] = 4. Commented out, because we want max num of threads, which is default\n",
    "params['scale_pos_weight'] = 1\n",
    "params['tree_method']= 'gpu_hist'\n",
    "params['gamma']= 0.4\n",
    "params['subsample']=0.8\n",
    "params['colsample_bytree']=0.8\n",
    "params['seed']=27\n",
    "params['scale_pos_weight']=1\n",
    "\n",
    "num_round= 20\n",
    "cv_folds = 5\n",
    "early_stopping_rounds=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from matrices\n",
    "check to make sure before training that the matrices look normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUMBER_OF_CAMPAIGNS_RECEIVED</th>\n",
       "      <th>MARKETING_PERMISSION</th>\n",
       "      <th>TELEPHONE_AND_MAIL_PERMISSION</th>\n",
       "      <th>DURATION_OF_OWNERSHIP</th>\n",
       "      <th>NUMBER_OF_CARS_OWNED_BEFORE</th>\n",
       "      <th>CAR_AGE</th>\n",
       "      <th>CAR_BOUGHT_AT_VW_DEALER</th>\n",
       "      <th>CAR_MODEL</th>\n",
       "      <th>CAR_PRICE</th>\n",
       "      <th>CO2_EMISSIONS</th>\n",
       "      <th>...</th>\n",
       "      <th>WARRANTY_COSTS_12M</th>\n",
       "      <th>AVG_DURATION</th>\n",
       "      <th>MILEAGE</th>\n",
       "      <th>NEXT_MOT</th>\n",
       "      <th>NUM_WORKSHOP_VISITS</th>\n",
       "      <th>NUM_WORKSHOP_VISITS_12M</th>\n",
       "      <th>SHARE_REPAIR_CASES</th>\n",
       "      <th>SHARE_REPAIR_CASES_12M</th>\n",
       "      <th>ENGINE_POWER_KW_1</th>\n",
       "      <th>ENGINE_POWER_COL1_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>7.50137</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>80439.296875</td>\n",
       "      <td>124.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.69565</td>\n",
       "      <td>433415</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>586</td>\n",
       "      <td>9</td>\n",
       "      <td>5.00274</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>39344.300781</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.28571</td>\n",
       "      <td>71986</td>\n",
       "      <td>-586.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1491</td>\n",
       "      <td>56919</td>\n",
       "      <td>5.50137</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>57900.000000</td>\n",
       "      <td>139.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.40000</td>\n",
       "      <td>119240</td>\n",
       "      <td>-307.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NUMBER_OF_CAMPAIGNS_RECEIVED  MARKETING_PERMISSION  \\\n",
       "0                             0                     0   \n",
       "1                             0                     0   \n",
       "2                             0                     0   \n",
       "\n",
       "   TELEPHONE_AND_MAIL_PERMISSION  DURATION_OF_OWNERSHIP  \\\n",
       "0                              0                     57   \n",
       "1                              0                    586   \n",
       "2                              0                   1491   \n",
       "\n",
       "   NUMBER_OF_CARS_OWNED_BEFORE  CAR_AGE  CAR_BOUGHT_AT_VW_DEALER  CAR_MODEL  \\\n",
       "0                            1  7.50137                        1          5   \n",
       "1                            9  5.00274                        1          2   \n",
       "2                        56919  5.50137                        1          5   \n",
       "\n",
       "      CAR_PRICE  CO2_EMISSIONS         ...           WARRANTY_COSTS_12M  \\\n",
       "0  80439.296875          124.0         ...                          0.0   \n",
       "1  39344.300781           99.0         ...                          0.0   \n",
       "2  57900.000000          139.0         ...                          0.0   \n",
       "\n",
       "   AVG_DURATION  MILEAGE  NEXT_MOT  NUM_WORKSHOP_VISITS  \\\n",
       "0       3.69565   433415      -4.0                   23   \n",
       "1       1.28571    71986    -586.0                    7   \n",
       "2       3.40000   119240    -307.0                    5   \n",
       "\n",
       "   NUM_WORKSHOP_VISITS_12M  SHARE_REPAIR_CASES  SHARE_REPAIR_CASES_12M  \\\n",
       "0                        4                   0                       0   \n",
       "1                        0                   0                       0   \n",
       "2                        0                   0                       0   \n",
       "\n",
       "   ENGINE_POWER_KW_1  ENGINE_POWER_COL1_0  \n",
       "0        -2147483648                  NaN  \n",
       "1        -2147483648                  NaN  \n",
       "2                 77                  1.6  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "dtype: int32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_y_pd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Copying Data: 0.8380823135375977\n",
      "Time taken for training: 2.9712088108062744\n"
     ]
    }
   ],
   "source": [
    "cpst= time.time()\n",
    "dtrain= xgb.DMatrix(df_train_pd,label=df_train_y_pd)\n",
    "cpet= time.time()\n",
    "print('Time taken for Copying Data: {}'.format(cpet-cpst))\n",
    "\n",
    "st= time.time()\n",
    "xmod= xgb.train(params,dtrain,num_round)\n",
    "en= time.time()\n",
    "print('Time taken for training: {}'.format(en-st))\n",
    "\n",
    "#cross-validation:\n",
    "cvresult = xgb.cv(params, dtrain, num_boost_round=num_round, nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds,show_stdv=True)\n",
    "ecvt = time.time()\n",
    "print('Time taken for cross validation: {}'.format(ecvt-en))\n",
    "\n",
    "#train again with cross-validation rounds:\n",
    "xmod= xgb.train(params,dtrain,cvresult.shape[0])\n",
    "\n",
    "err_val= xmod.eval(dtrain)\n",
    "print('Validation Accuracy: {}'.format(err_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize']= [15,12]\n",
    "matplotlib.rcParams['figure.dpi']= 55\n",
    "plot_importance(xmod)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt= time.time()\n",
    "pred_val= xmod.predict(xgb.DMatrix(df_test_pd))\n",
    "ept= time.time()\n",
    "err_pred= xmod.eval(xgb.DMatrix(df_test_pd,df_test_y_pd))\n",
    "\n",
    "print('Time taken for Predictions: {}'.format(ept-spt))\n",
    "print('Predictions Accuracy: {}'.format(err_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependency is a measure of how dependent set is on a certain feature. for more information, please visit: (https://cran.r-project.org/web/packages/datarobot/vignettes/PartialDependence.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the top 10 features for Partial Dependency grid creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var= 10  # Number of variables to show\n",
    "varimp= xmod.get_fscore()\n",
    "varimp= sorted(varimp.items(),key=lambda val: val[1],reverse=True)\n",
    "pdf_varimp= pd.DataFrame(varimp).iloc[0:var,:]\n",
    "#save the top 10 most important features to g\n",
    "gridCols = pdf_varimp[0].tolist()\n",
    "pdf_varimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_dependency(df,gridCols,percentiles=(0.02,0.98),resolution=100,g_o_c='cpu'):\n",
    "    '''\n",
    "    FUNCTION\n",
    "    Calculates the partial dependence values for a pandas datafame without requiring to create a grid, scikitlearn would do.\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    - df: this is the pandas dataframe being sent in\n",
    "    - gridCols: the columns being used for partial dependency\n",
    "    - percentiles: percent above and below the min that should be used for calculating partial dependency\n",
    "    - resolution: the amount of data points needed to interpolate between min value and max value\n",
    "    - g_o_c: whether the prediction should be done on the gpu or cpu\n",
    "    \n",
    "    OUTPUT:\n",
    "    \n",
    "    - par_dep_df: Returns a dataframe with the x-values and partial dependency values for all columns\n",
    "    \n",
    "    eg. par_dep_df = ['COLUMN_NAME' , 'X_VALUES', 'PARTIAL_DEPENDENCY_VALUES']\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #check to make sure percentiles entered in correctly:\n",
    "    if len(percentiles) != 2:\n",
    "        raise ValueError('percentile must be tuple of len 2')\n",
    "    if not all(0. <= x <= 1. for x in percentiles):\n",
    "        raise ValueError('percentile values must be in [0, 1]')\n",
    "    \n",
    "    #initialize array:\n",
    "    out_values = []\n",
    "    gridCols = ['CAR_PRICE']\n",
    "    #print total size of p_d dataframe:\n",
    "    print('# of rows in partial dependence df: ', resolution*len(gridCols))\n",
    "    \n",
    "    #set the model to be trained on GPU or CPU\n",
    "    if g_o_c is 'gpu':\n",
    "        xmod.set_param({\"predictor\":\"gpu_predictor\"})\n",
    "    else:\n",
    "        xmod.set_param({\"predictor\":\"cpu_predictor\"})\n",
    "    \n",
    "    for feat in gridCols:\n",
    "        print('Calculating for feature: ', feat)\n",
    "    \n",
    "        #Calculate the percentile values for the feature:\n",
    "        uniq = df[feat].unique()\n",
    "        \n",
    "        #calculate how to make 1D grid:\n",
    "        if len(uniq) > resolution: \n",
    "            q = df[feat].quantile(q = percentiles)\n",
    "            axis= np.linspace(q.min(),q.max(),num=resolution, endpoint=True)\n",
    "            \n",
    "            if df[feat].dtype.kind == 'i':\n",
    "                axis = np.round(axis).astype('int')\n",
    "        else:\n",
    "            axis = np.sort(uniq.tolist(),axis=0)\n",
    "        \n",
    "        axis = axis[~np.isnan(axis)]\n",
    "        #save df_train_pd as grid\n",
    "        df_copy = deepcopy(df)\n",
    "\n",
    "        #calculate partial dependency for each value of i in the linearly spaced 1D grid:\n",
    "        for i in axis:\n",
    "            #print(i)\n",
    "            df_copy[feat] = i\n",
    "            t1 = time.time()\n",
    "            pred = xmod.predict(xgb.DMatrix(df_copy))\n",
    "            tf = time.time()-t1\n",
    "            print('time to predict: ', tf)\n",
    "            p_d = ( ( 1 / df_copy.shape[0] ) * sum(pred)) #calculation of partial dependency\n",
    "            out_values.append([feat, i , p_d])\n",
    "\n",
    "    #save in to dataframe:\n",
    "    par_dep_df = pd.DataFrame(out_values, columns=['feature', 'x', 'p_d'])\n",
    "    \n",
    "    return par_dep_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#put a comment in here about the percentile (to remove wild values, std deviation) \n",
    "resolution = 3\n",
    "par_dep_df=partial_dependency(df_train_pd,gridCols,(0.05,0.90),resolution,'gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dep_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Partial Dependence Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get unique values of features:\n",
    "features_pardep = par_dep_df['feature'].unique()\n",
    "for feat in features_pardep:\n",
    "    plt.figure()    \n",
    "    df_tmp = par_dep_df[par_dep_df['feature']==feat]\n",
    "    df_tmp.plot(x='x', y='p_d')\n",
    "    plt.title(feat)\n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel('Partial Dependence')\n",
    "    plt.yticks(np.arange(0.15, 0.4, 0.03))\n",
    "    #plt.set_ylim(0, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: \n",
    "\n",
    " Predict on whole data set using model, xmod. This value is p_all\n",
    "\n",
    "Pre step: Calculate p_all\n",
    "\n",
    "1. Set a feature to NULL (essentially remove it from the model)\n",
    "2. Predict the values for the data set with one feature null p_i\n",
    "3. calculate difference between p_all and p_i. This will be a measure of importance p_importance = p_i - p_all\n",
    "\n",
    "### Create Dataframes\n",
    "- Create dataframe for variable importance matrix\n",
    "- Create dataframe for difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_importance(df, index_1,g_o_c='cpu'):\n",
    "    '''\n",
    "    var_importance calculates the variable importance for each column.\n",
    "    in this context, the variable importance for each record is defined as:\n",
    "    \n",
    "    Which feature changes the prediction value for a record \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    features = df.columns.tolist()\n",
    "    #temporary array used to store the data, to then be made to pandas.\n",
    "    tmp_array = []\n",
    "    \n",
    "    if g_o_c is 'gpu':\n",
    "        xmod.set_param({\"predictor\":\"gpu_predictor\"})\n",
    "    else:\n",
    "        xmod.set_param({\"predictor\":\"cpu_predictor\"})\n",
    "    \n",
    "    #Calculate base-case of probabilities, p_all\n",
    "    p_all = xmod.predict( xgb.DMatrix(df) )\n",
    "    s_p_all = pd.Series(p_all)\n",
    "\n",
    "        #calculate variable importance:\n",
    "    for feat in features:\n",
    "        print('\\n Current Feature: ' ,feat)\n",
    "\n",
    "        t1 = time.time()\n",
    "        df_vimp        = deepcopy(df_train_pd)\n",
    "        df_vimp[feat]  =  np.nan\n",
    "\n",
    "        t2 = time.time()\n",
    "        pred_varimp    = xmod.predict(xgb.DMatrix(df_vimp))  \n",
    "        t3             = time.time()\n",
    "        \n",
    "        print('time to predict: ', t3-t2)\n",
    "        \n",
    "        print('\\n')\n",
    "        tmp_array.append(pred_varimp)\n",
    "\n",
    "        #Create variable importance Dataframe:\n",
    "    var_imp_val             = list(map(list, zip(*tmp_array)))\n",
    "    df_var_imp_no_index     = pd.DataFrame(var_imp_val, columns = features)\n",
    "    df_var_imp              = pd.concat([index_1,df_var_imp_no_index], axis=1)\n",
    "    df_var_imp              = df_var_imp.set_index(index_1.columns.tolist())\n",
    "\n",
    "    #Create difference dataframe with (p_all - p_i) values:\n",
    "    df_vi_diff_no_index     = abs(df_var_imp_no_index.sub(s_p_all,axis=0))\n",
    "    df_vi_diff              = pd.concat([index_1,df_vi_diff_no_index], axis=1)\n",
    "    df_vi_diff              = df_vi_diff.set_index(index_1.columns.tolist())\n",
    "    \n",
    "    return df_var_imp, df_vi_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_var_imp, df_vi_diff = var_importance(df_train_pd,index_df[:cp],'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort Variable importancee by top values for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorter(row):\n",
    "    row = row.sort_values(ascending=False)\n",
    "    return list(zip(row.index, row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = deepcopy(df_vi_diff)\n",
    "top_vals = a.head().apply(lambda row: sorter(row), axis=1)\n",
    "\n",
    "#number zof top values to keep:\n",
    "rank = 5\n",
    "\n",
    "#reassign column names to the number:\n",
    "#for col in top_vals.columns:\n",
    "    \n",
    "#remove all other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_vals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the top values dataframe\n",
    "\n",
    "This dataframe contains the most important features, per row (per record)\n",
    "\n",
    "eg: top_vals = (index:ID and VIN) , columns:[1st,2nd,3rd,4th,5th]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading New Data to MapD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe for Predicted values "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
