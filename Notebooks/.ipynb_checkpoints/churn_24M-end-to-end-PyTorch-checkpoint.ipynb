{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Dataset : PyMapD- PyTorch- MapD\n",
    "# Response Variable: Evasion_24M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymapd\n",
    "import pygdf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "le= LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up MapD connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection(mapd://mapd:***@http://localhost:9090/mapd?protocol=http)\n"
     ]
    }
   ],
   "source": [
    "dbname    = 'mapd'\n",
    "username  = 'mapd'\n",
    "password  = 'HyperInteractive'\n",
    "hostname  = 'localhost'\n",
    "port      = 9090\n",
    "\n",
    "con = pymapd.connect(user=username,password=password,dbname=dbname,host=hostname,port=port,protocol='http')\n",
    "print(con)\n",
    "c   = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from MapD to PyGDF\n",
    "\n",
    "#### Create Table evasion_v2, and load data into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!cd /tmp/my_docker/jupyter_env/scripts && ls && ./create_evasion_table_no_extras.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 49\n"
     ]
    }
   ],
   "source": [
    "table= 'evasion_v2'\n",
    "response= 'EVASION_FLAG_24M'\n",
    "\n",
    "date_column = '''REFERENCE_DATE'''\n",
    "\n",
    "columns= '''PRIVATE_CUSTOMER ,TRAIN_TEST ,NUMBER_OF_CAMPAIGNS_RECEIVED ,MARKETING_PERMISSION ,TELEPHONE_AND_MAIL_PERMISSION ,DURATION_OF_OWNERSHIP ,NUMBER_OF_CARS_OWNED_BEFORE ,CAR_AGE ,CAR_BOUGHT_AT_VW_DEALER ,CAR_MODEL ,CAR_PRICE ,CO2_EMISSIONS ,PRODUCTION_YEAR ,EXTENDED_WARRANTY ,SERVICE_AND_MAINTEN_PACKAGE ,WARRANTY_LEFT ,ECONOMY_PARTS_12M ,MAINTENANCE_COSTS ,MAINTENANCE_COSTS_12M ,NUM_MAINTENANCE ,NUM_MAINTENANCE_12M , NUM_REPAIRS ,NUM_REPAIRS_12M ,NUM_WARRANTY ,REPAIR_COSTS ,REPAIR_COSTS_12M ,SERVICE_COSTS ,SERVICE_COSTS_12M ,TOTAL_COSTS ,WARRANTY_COSTS ,WARRANTY_COSTS_12M ,AVG_DURATION ,MILEAGE ,NEXT_MOT ,NUM_WORKSHOP_VISITS ,NUM_WORKSHOP_VISITS_12M ,SHARE_REPAIR_CASES ,SHARE_REPAIR_CASES_12M ,VIN_HASHED ,CUSTOMER_ID_HASHED ,MODEL_CODE,ENGINE_POWER ,ENGINE_POWER_KW_0  ,ENGINE_POWER_KW_1 ,ENGINE_POWER_COL1_0 ,HORSE_POWER  ,HORSE_POWER_0  ,HORSE_POWER_1'''\n",
    "columns_str= '''CAR_MODEL,PRODUCTION_YEAR'''\n",
    "\n",
    "\n",
    "print('Number of Columns: %d'%(len((columns+','+response).split(','))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Pre-Processing (DF_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unwanted columns/ Remove rowid \n",
    "\n",
    "\n",
    "### comments:\n",
    "\n",
    "**THIS PART IS DONE ON ALL THE DATA (HENCE THE DF_ALL DATAFRAME)**\n",
    "\n",
    "### TO-DO:\n",
    "- Analyze the data on the gpu (using the pygdf), and make on the fly corrections here on the jupyter notebook.\n",
    "- From the cleaned data, save the data to df_train\n",
    "\n",
    "**Remove these columns, (From Asghar's r file):**\n",
    "- reference_date (already removed)\n",
    "- private_customer\n",
    "- evasion flags\n",
    "- train_test\n",
    "- customer_id_hashed\n",
    "- vin_hashed\n",
    "- engine_power\n",
    "- engine power_kW_0\n",
    "- horse power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DF_Train and DF_Test\n",
    "\n",
    "**As of now, this part is created from the original data. It is not based on the data pre-processing done above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Training set: 1424232\n",
      "Number of rows in Test set: 357019\n"
     ]
    }
   ],
   "source": [
    "query_train = '''Select {},{} from {} Where train_test='train' '''.format(response,columns,table)\n",
    "query_test = '''Select {},rowid AS mapid,{} from {} Where train_test='test' '''.format(response,columns,table)\n",
    "\n",
    "# implicit tdf to pygdf\n",
    "df_train = con.select_ipc_gpu(query_train,device_id=0)\n",
    "df_test  = con.select_ipc_gpu(query_test,device_id=0)\n",
    "\n",
    "#df_train = con.select_ipc(query_train)\n",
    "#df_test  = con.select_ipc(query_test)\n",
    "\n",
    "\n",
    "#print for row_size check:\n",
    "print('Number of rows in Training set: %d'%(len(df_train)))\n",
    "print('Number of rows in Test set: %d'%(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','VIN_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "#rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "\n",
    "vin_hash_series = df_train['VIN_HASHED'].to_pandas()\n",
    "\n",
    "for col in rm_cols:\n",
    "    df_train.drop_column(col)\n",
    "    df_test.drop_column(col)\n",
    "    #df_all.drop_column(col)\n",
    "    \n",
    "df_pred=pygdf.DataFrame()\n",
    "df_pred.add_column('mapid', df_test['mapid'])\n",
    "df_test.drop_column('mapid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cat cols : Labels Encoding\n",
    "#### Comments:\n",
    "\n",
    "fillna, what does it do? does this mask null with the value -1? how could this affect the ML process?\n",
    "\n",
    "<span style=\"color:green\"> **the -1 is used for label encoding. this is only for string columns** </span>\n",
    "\n",
    "needs to be changed to pygdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR_MODEL\n",
      "PRODUCTION_YEAR\n"
     ]
    }
   ],
   "source": [
    "for col in columns_str.split(','):\n",
    "    ctrain= df_train[col].fillna(-1).to_pandas()\n",
    "    ctest= df_test[col].fillna(-1).to_pandas()\n",
    "    fit= le.fit(ctrain.astype(str))\n",
    "    df_train[col] = fit.transform(ctrain.astype(str))\n",
    "    print(col)\n",
    "    df_test[col] = fit.transform(ctest.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = set(df_train.columns)\n",
    "features = columns - set([response])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fill na/nan\n",
    "\n",
    "### Comments:\n",
    "* Why are na values being filled with -999? How does the model handle -999 values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    df_train[col] = df_train[col].fillna(df_train[col].mean())   # Treat missing values\n",
    "    df_test[col] = df_test[col].fillna(df_test[col].mean())\n",
    "    df_train[col] = df_train[col].astype(np.float32) # Make consistent datatype\n",
    "    df_test[col] = df_test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EVASION_FLAG_24M                   int32\n",
       "NUMBER_OF_CAMPAIGNS_RECEIVED     float32\n",
       "MARKETING_PERMISSION             float32\n",
       "TELEPHONE_AND_MAIL_PERMISSION    float32\n",
       "DURATION_OF_OWNERSHIP            float32\n",
       "NUMBER_OF_CARS_OWNED_BEFORE      float32\n",
       "CAR_AGE                          float32\n",
       "CAR_BOUGHT_AT_VW_DEALER          float32\n",
       "CAR_MODEL                        float32\n",
       "CAR_PRICE                        float32\n",
       "CO2_EMISSIONS                    float32\n",
       "PRODUCTION_YEAR                  float32\n",
       "EXTENDED_WARRANTY                float32\n",
       "SERVICE_AND_MAINTEN_PACKAGE      float32\n",
       "WARRANTY_LEFT                    float32\n",
       "ECONOMY_PARTS_12M                float32\n",
       "MAINTENANCE_COSTS                float32\n",
       "MAINTENANCE_COSTS_12M            float32\n",
       "NUM_MAINTENANCE                  float32\n",
       "NUM_MAINTENANCE_12M              float32\n",
       "NUM_REPAIRS                      float32\n",
       "NUM_REPAIRS_12M                  float32\n",
       "NUM_WARRANTY                     float32\n",
       "REPAIR_COSTS                     float32\n",
       "REPAIR_COSTS_12M                 float32\n",
       "SERVICE_COSTS                    float32\n",
       "SERVICE_COSTS_12M                float32\n",
       "TOTAL_COSTS                      float32\n",
       "WARRANTY_COSTS                   float32\n",
       "WARRANTY_COSTS_12M               float32\n",
       "AVG_DURATION                     float32\n",
       "MILEAGE                          float32\n",
       "NEXT_MOT                         float32\n",
       "NUM_WORKSHOP_VISITS              float32\n",
       "NUM_WORKSHOP_VISITS_12M          float32\n",
       "SHARE_REPAIR_CASES               float32\n",
       "SHARE_REPAIR_CASES_12M           float32\n",
       "ENGINE_POWER_KW_1                float32\n",
       "ENGINE_POWER_COL1_0              float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilization of PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_y= df_train[response]\n",
    "df_test_y= df_test[response]\n",
    "\n",
    "del df_train[response]\n",
    "del df_test[response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pygdf.dataframe.DataFrame'>\n",
      "(1424232, 38)\n",
      "<class 'numba.cuda.cudadrv.devicearray.DeviceNDArray'>\n"
     ]
    }
   ],
   "source": [
    "# convert the gdf to a gpu matrix\n",
    "print(type(df_train))\n",
    "m = df_train.as_gpu_matrix()\n",
    "print(m.shape)\n",
    "print(type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "import ctypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple cuda copy kernel\n",
    "@cuda.jit\n",
    "def test(in_array, out_ptr):\n",
    "  x, y = cuda.grid(2)\n",
    "  if x < in_array.shape[0] and y < in_array.shape[1]:\n",
    "    out_ptr[x, y] = in_array[x, y]\n",
    "    \n",
    "# setting up grid and block sizes\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = math.ceil(m.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(m.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# TODO: this is hardcoded for 2D tensor for now\n",
    "# I tried torch.LongStorage({m.shape[0],m.shape[1]}) but it didn't work\n",
    "t = torch.cuda.FloatTensor(m.shape[0], m.shape[1])\n",
    "print(t.size())\n",
    "ptr = t.data_ptr()\n",
    "ctx = cuda.current_context()\n",
    "# memory pointer object for tensor data\n",
    "memptr = cuda.cudadrv.driver.MemoryPointer(ctx, ctypes.c_uint64(ptr), t.numel() * t.element_size())\n",
    "# ndarray representation of our tensor \n",
    "arr = cuda.devicearray.DeviceNDArray(shape=m.shape, strides=tuple(4*x for x in t.stride()), dtype=np.dtype(np.float32), gpu_data=memptr)\n",
    "# plaunch cuda kernel\n",
    "test[blockspergrid, threadsperblock](m, arr)\n",
    "#\n",
    "# check output values\n",
    "m_host = m.copy_to_host()\n",
    "#for x in range(m.shape[0]):\n",
    "#    for y in range(m.shape[1]):\n",
    "#        print(x)\n",
    "#        assert(m_host[x, y] == t[x, y])\n",
    "#print(m_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "torch.is_tensor(t)\n",
    "print(t[5][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make label\n",
    "label=df_train_y.to_array()\n",
    "label=torch.from_numpy(label)\n",
    "\n",
    "#make features\n",
    "#features=df.as_matrix()\n",
    "#features=torch.from_numpy(features)\n",
    "\n",
    "# clean up the IPC handle\n",
    "#ipch.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_torch = Variable(t)\n",
    "train_y_torch = Variable(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data in MapD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe for Predicted values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predcol= response+'_pred'\n",
    "# predtab= table+'_predictions'\n",
    "# predview= predtab+'_view'\n",
    "\n",
    "# pdf_pred = df_pred.to_pandas()\n",
    "# # pdf_pred.reset_index(inplace=True, drop=True)\n",
    "# pdf_pred[predcol] = pd.DataFrame(pred_val[np.newaxis][0].T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdf_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table/view for predictions in MapD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query_pred_drop = 'DROP TABLE IF EXISTS {};'.format(predtab)\n",
    "# query_pred_create = 'CREATE TABLE IF NOT EXISTS {}({} BIGINT NOT NULL, {} FLOAT);'.format(\n",
    "#     predtab, 'mapid', predcol)\n",
    "# query_view_drop = 'DROP VIEW IF EXISTS {};'.format(predview)\n",
    "# query_view = \"CREATE VIEW {} AS (select a.*,b.{} from {} a LEFT JOIN {} b ON a.{} = b.{});\".format(\n",
    "#     predview, predcol, table, predtab, 'rowid', 'mapid')\n",
    "\n",
    "# cur = con.cursor()\n",
    "# cur.execute(query_pred_drop)\n",
    "# cur.execute(query_pred_create)\n",
    "# cur.execute(query_view_drop)\n",
    "# # cur.execute('drop view if exists churn_predictions_view')\n",
    "# # cur.execute('drop table if exists churn_predictions')\n",
    "# cur.execute(query_view)\n",
    "\n",
    "# cur.close()   # close the cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lst= time.time()\n",
    "# con.load_table(predtab,pdf_pred.itertuples(index=False))\n",
    "# let= time.time()\n",
    "\n",
    "# print('Total time taken to load the records {}'.format(let-lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con.get_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
