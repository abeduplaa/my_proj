{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Dataset : PyMapD- XGBoost - MapD\n",
    "# Response Variable: Evasion_24M\n",
    "## No copying to the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymapd\n",
    "import pygdf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "le= LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up MapD connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection(mapd://mapd:***@http://localhost:9090/mapd?protocol=http)\n"
     ]
    }
   ],
   "source": [
    "dbname    = 'mapd'\n",
    "username  = 'mapd'\n",
    "password  = 'HyperInteractive'\n",
    "hostname  = 'localhost'\n",
    "port      = 9090\n",
    "\n",
    "con = pymapd.connect(user=username,password=password,dbname=dbname,host=hostname,port=port,protocol='http')\n",
    "print(con)\n",
    "c   = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from MapD to PyGDF\n",
    "\n",
    "#### Create Table evasion_v2, and load data into table\n",
    "* There is a script that I use to load the data on to mapd\n",
    "* However, there are multiple different ways to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!cd /tmp/my_docker/jupyter_env/scripts && ls && ./create_evasion_table_no_extras.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 49\n"
     ]
    }
   ],
   "source": [
    "table= 'evasion_v2'\n",
    "response= 'EVASION_FLAG_24M'\n",
    "\n",
    "date_column = '''REFERENCE_DATE'''\n",
    "\n",
    "columns= '''PRIVATE_CUSTOMER ,TRAIN_TEST ,NUMBER_OF_CAMPAIGNS_RECEIVED ,MARKETING_PERMISSION ,TELEPHONE_AND_MAIL_PERMISSION ,DURATION_OF_OWNERSHIP ,NUMBER_OF_CARS_OWNED_BEFORE ,CAR_AGE ,CAR_BOUGHT_AT_VW_DEALER ,CAR_MODEL ,CAR_PRICE ,CO2_EMISSIONS ,PRODUCTION_YEAR ,EXTENDED_WARRANTY ,SERVICE_AND_MAINTEN_PACKAGE ,WARRANTY_LEFT ,ECONOMY_PARTS_12M ,MAINTENANCE_COSTS ,MAINTENANCE_COSTS_12M ,NUM_MAINTENANCE ,NUM_MAINTENANCE_12M , NUM_REPAIRS ,NUM_REPAIRS_12M ,NUM_WARRANTY ,REPAIR_COSTS ,REPAIR_COSTS_12M ,SERVICE_COSTS ,SERVICE_COSTS_12M ,TOTAL_COSTS ,WARRANTY_COSTS ,WARRANTY_COSTS_12M ,AVG_DURATION ,MILEAGE ,NEXT_MOT ,NUM_WORKSHOP_VISITS ,NUM_WORKSHOP_VISITS_12M ,SHARE_REPAIR_CASES ,SHARE_REPAIR_CASES_12M ,VIN_HASHED ,CUSTOMER_ID_HASHED ,MODEL_CODE,ENGINE_POWER ,ENGINE_POWER_KW_0  ,ENGINE_POWER_KW_1 ,ENGINE_POWER_COL1_0 ,HORSE_POWER  ,HORSE_POWER_0  ,HORSE_POWER_1'''\n",
    "columns_str= '''CAR_MODEL,PRODUCTION_YEAR'''\n",
    "\n",
    "index_columns = '''CUSTOMER_ID_HASHED,VIN_HASHED'''\n",
    "\n",
    "print('Number of Columns: %d'%(len((columns+','+response).split(','))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DF_Train and DF_Test\n",
    "\n",
    "**As of now, this part is created from the original data. It is not based on the data pre-processing done above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in Training set: 1424232\n",
      "Number of rows in Test set: 357019\n"
     ]
    }
   ],
   "source": [
    "query_train = '''Select {},{} from {} Where train_test='train' '''.format(response,columns,table)\n",
    "query_test = '''Select {},rowid AS mapid,{} from {} Where train_test='test' '''.format(response,columns,table)\n",
    "\n",
    "query_index = '''Select {} from {} Where train_test='train' '''.format(index_columns,table)\n",
    "\n",
    "# implicit tdf to pygdf\n",
    "df_train = con.select_ipc_gpu(query_train,device_id=0)\n",
    "df_test  = con.select_ipc_gpu(query_test,device_id=0)\n",
    "\n",
    "index_df = con.select_ipc_gpu(query_index)\n",
    "index_df = index_df.to_pandas()\n",
    "#print for row_size check:\n",
    "print('Number of rows in Training set: %d'%(len(df_train)))\n",
    "print('Number of rows in Test set: %d'%(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','VIN_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "#rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "\n",
    "#vin_hash_series = df_train['VIN_HASHED'].to_pandas()\n",
    "\n",
    "for col in rm_cols:\n",
    "    df_train.drop_column(col)\n",
    "    df_test.drop_column(col)\n",
    "    #df_all.drop_column(col)\n",
    "    \n",
    "df_pred=pygdf.DataFrame()\n",
    "df_pred.add_column('mapid', df_test['mapid'])\n",
    "df_test.drop_column('mapid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cat cols : Labels Encoding\n",
    "#### Comments:\n",
    "\n",
    "fillna, what does it do? does this mask null with the value -1? how could this affect the ML process?\n",
    "\n",
    "<span style=\"color:green\"> **the -1 is used for label encoding. this is only for string columns** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR_MODEL\n",
      "PRODUCTION_YEAR\n"
     ]
    }
   ],
   "source": [
    "for col in columns_str.split(','):\n",
    "    ctrain= df_train[col].fillna(-1).to_pandas()\n",
    "    ctest= df_test[col].fillna(-1).to_pandas()\n",
    "    fit= le.fit(ctrain.astype(str))\n",
    "    df_train[col] = fit.transform(ctrain.astype(str))\n",
    "    print(col)\n",
    "    df_test[col] = fit.transform(ctest.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = set(df_train.columns)\n",
    "features = columns - set([response])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fill na/nan\n",
    "\n",
    "### Comments:\n",
    "* Why are na values being filled with -999? How does the model handle -999 values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    df_train[col] = df_train[col].fillna(np.nan)   # Treat missing values\n",
    "    df_test[col] = df_test[col].fillna(np.nan)\n",
    "    df_train[col] = df_train[col].astype(np.float32) # Make consistent datatype\n",
    "    df_test[col] = df_test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_y= df_train[response]\n",
    "df_test_y= df_test[response]\n",
    "\n",
    "del df_train[response]\n",
    "del df_test[response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GPU DF/matrices of Training;Val;Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    df_train[col] = df_train[col].astype(np.float32) # Make consistent datatype\n",
    "    df_test[col] = df_test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pygdf.dataframe.DataFrame'>\n",
      "(1424232, 38)\n",
      "<class 'numba.cuda.cudadrv.devicearray.DeviceNDArray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_train))\n",
    "m = df_train.as_gpu_matrix()\n",
    "print(m.shape)\n",
    "print(type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x7fe6a599f3c8\n",
      "<xgboost.core.DMatrix object at 0x7fe6a599f3c8>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DMatrix' object has no attribute 'data_ptr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d3ad513dd199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DMatrix' object has no attribute 'data_ptr'"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "import ctypes\n",
    "\n",
    "# simple cuda copy kernel\n",
    "@cuda.jit\n",
    "def test(in_array, out_ptr):\n",
    "  x, y = cuda.grid(2)\n",
    "  if x < in_array.shape[0] and y < in_array.shape[1]:\n",
    "    out_ptr[x, y] = in_array[x, y]\n",
    "    \n",
    "# setting up grid and block sizes\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = math.ceil(m.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(m.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "m_xgb = xgb.DMatrix(np.random.rand(m.shape[0], m.shape[1]),label=np.random.randn(m.shape[0],2))\n",
    "print (hex(id(m_xgb)))\n",
    "print(m_xgb)\n",
    "ptr = m_xgb.data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters\n",
    "\n",
    "### Comments:\n",
    "<span style=\"color:red\"> **We need to tune the parameters here. I'm not sure if these parameters are optimal. We should look in to how to optimize parameters** </span>\n",
    "\n",
    "* How was the performance metric done in the UC previously?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params= {}\n",
    "params['objective']= 'binary:logistic'\n",
    "params['eval_metric']= 'auc'\n",
    "params['max_depth']= 7\n",
    "params['eta']= 0.3\n",
    "params['silent']= 0\n",
    "#params['tree_method']= 'gpu_exact'\n",
    "params['tree_method']= 'gpu_hist'\n",
    "\n",
    "num_round= 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from matrices\n",
    "check to make sure before training that the matrices look normal\n",
    "Convert to xgboost dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_pd = df_train_n.to_pandas()\n",
    "df_val_pd = df_val.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in df_train_pd.columns:\n",
    "    if df_train_pd[x].std(axis=0) > 10:\n",
    "        print(x, df_train_pd[x].dtype,df_train_pd[x].max(axis=0), df_train_pd[x].std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cpst= time.time()\n",
    "dtrain= xgb.DMatrix(df_train_pd,label=df_train_y.to_pandas())\n",
    "cpet= time.time()\n",
    "print('Time taken for Copying Data: {}'.format(cpet-cpst))\n",
    "\n",
    "st= time.time()\n",
    "xmod= xgb.train(params,dtrain,num_round)\n",
    "en= time.time()\n",
    "print('Time taken for training: {}'.format(en-st))\n",
    "\n",
    "err_val= xmod.eval(xgb.DMatrix(df_val_pd,label=df_val_y.to_pandas()))\n",
    "print('Validation Accuracy: {}'.format(err_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize']= [15,12]\n",
    "matplotlib.rcParams['figure.dpi']= 55\n",
    "plot_importance(xmod)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spt= time.time()\n",
    "pred_val= xmod.predict(xgb.DMatrix(df_test.to_pandas()))\n",
    "ept= time.time()\n",
    "err_pred= xmod.eval(xgb.DMatrix(df_test.to_pandas(),df_test_y.to_pandas()))\n",
    "\n",
    "print('Time taken for Predictions: {}'.format(ept-spt))\n",
    "print('Predictions Accuracy: {}'.format(err_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top features for grid creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var= 10  # Number of variables to show\n",
    "varimp= xmod.get_fscore()\n",
    "varimp= sorted(varimp.items(),key=lambda val: val[1],reverse=True)\n",
    "pdf_varimp= pd.DataFrame(varimp).iloc[0:var,:]\n",
    "pdf_varimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridCols = pdf_varimp[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create Partial Dependency grid\n",
    "\n",
    "- Potential issue with -999 as null values in grid creation. may have to switch them back to Null or another value?\n",
    "- also, should there be a grid for each feature? \n",
    "\n",
    "\n",
    "### Calculate Partial Dependency\n",
    "\n",
    "1. the min and max values of the feature are saved to f_min and f_max\n",
    "2. xi is the array of values that are being used for calculation of the Partial Dependency Analysis\n",
    "\n",
    "####  Potential Problems:\n",
    "\n",
    "- Using -999 as null values. Create\n",
    "- how to store all the values?\n",
    "- What would be the total amount of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_dependency(df,gridCols,percentiles=(0.02,0.98),resolution=100,g_o_c='cpu'):\n",
    "    '''\n",
    "    Calculate the partial dependence values for a pandas datafame without requiring to create a grid.\n",
    "    \n",
    "    '''\n",
    "    #check to make sure percentiles entered in correctly:\n",
    "    if len(percentiles) != 2:\n",
    "        raise ValueError('percentile must be tuple of len 2')\n",
    "    if not all(0. <= x <= 1. for x in percentiles):\n",
    "        raise ValueError('percentile values must be in [0, 1]')\n",
    "    \n",
    "    #initialize array:\n",
    "    out_values = []\n",
    "    \n",
    "    #print total size of p_d dataframe:\n",
    "    print('# of rows in partial dependence df: ', resolution*len(gridCols))\n",
    "    \n",
    "    #set the model to be trained on GPU or CPU\n",
    "    if g_o_c is 'gpu':\n",
    "        xmod.set_param({\"predictor\":\"gpu_predictor\"})\n",
    "    else:\n",
    "        xmod.set_param({\"predictor\":\"cpu_predictor\"})\n",
    "    \n",
    "    for feat in gridCols:\n",
    "        print('Calculating for feature: ', feat)\n",
    "    \n",
    "        #Calculate the percentile values for the feature:\n",
    "        uniq = df[feat].unique()\n",
    "        \n",
    "        #calculate how to make 1D grid:\n",
    "        if len(uniq) > resolution: \n",
    "            q = df[feat].quantile(q = percentiles)\n",
    "            axis= np.linspace(q.min(),q.max(),num=resolution, endpoint=True)\n",
    "            \n",
    "            if df[feat].dtype.kind == 'i':\n",
    "                axis = np.round(axis).astype('int')\n",
    "        else:\n",
    "            axis = np.sort(uniq.tolist(),axis=0)\n",
    "        \n",
    "        axis = axis[~np.isnan(axis)]\n",
    "        #save df_train_pd as grid\n",
    "        df_copy = deepcopy(df)\n",
    "\n",
    "        #calculate partial dependency for each value of i in the linearly spaced 1D grid:\n",
    "        for i in axis:\n",
    "            #print(i)\n",
    "            #look in to using sci-kit learn partial dependency. Compare with sci-kit partial dependency calculation.\n",
    "            #if there is a big difference, sci-kit learn may be outputting difference between regular.\n",
    "            df_copy[feat] = i\n",
    "            t1 = time.time()\n",
    "            pred = xmod.predict(xgb.DMatrix(df_copy))\n",
    "            tf = time.time()-t1\n",
    "            print('time to predict: ', tf)\n",
    "            p_d = ( ( 1 / df_copy.shape[0] ) * sum(pred)) #calculation of partial dependency\n",
    "            out_values.append([feat, i , p_d])\n",
    "\n",
    "    #save in to dataframe:\n",
    "    par_dep_df = pd.DataFrame(out_values, columns=['feature', 'x', 'p_d'])\n",
    "    \n",
    "    return par_dep_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "par_dep_df=partial_dependency(df_train_pd,gridCols,(0.05,0.90),resolution,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par_dep_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Partial Dependence Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get unique values of features:\n",
    "features_pardep = par_dep_df['feature'].unique()\n",
    "for feat in features_pardep:\n",
    "    plt.figure()    \n",
    "    df_tmp = par_dep_df[par_dep_df['feature']==feat]\n",
    "    df_tmp.plot(x='x', y='p_d')\n",
    "    plt.title(feat)\n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel('Partial Dependence')\n",
    "    plt.yticks(np.arange(0.15, 0.4, 0.03))\n",
    "    #plt.set_ylim(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train_pd.shape[0]\n",
    "df_train_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: \n",
    "\n",
    " Predict on whole data set using model, xmod. This value is p_all\n",
    "\n",
    "Pre step: Calculate p_all\n",
    "\n",
    "1. Set a feature to NULL (essentially remove it from the model)\n",
    "2. Predict the values for the data set with one feature null p_i\n",
    "3. calculate difference between p_all and p_i. This will be a measure of importance p_importance = p_i - p_all\n",
    "\n",
    "### Create Dataframes\n",
    "- Create dataframe for variable importance matrix\n",
    "- Create dataframe for difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_importance(df, index_1,g_o_c='cpu'):\n",
    "    '''\n",
    "    var_importance calculates the variable importance for each column.\n",
    "    in this context, the variable importance for each record is defined as:\n",
    "    \n",
    "    Which feature changes the prediction value for a record \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    features = df.columns.tolist()\n",
    "    #temporary array used to store the data, to then be made to pandas.\n",
    "    tmp_array = []\n",
    "    \n",
    "    if g_o_c is 'gpu':\n",
    "        xmod.set_param({\"predictor\":\"gpu_predictor\"})\n",
    "    else:\n",
    "        xmod.set_param({\"predictor\":\"cpu_predictor\"})\n",
    "    \n",
    "    #Calculate base-case of probabilities, p_all\n",
    "    p_all = xmod.predict( xgb.DMatrix(df) )\n",
    "    s_p_all = pd.Series(p_all)\n",
    "\n",
    "        #calculate variable importance:\n",
    "    for feat in features:\n",
    "        print('\\n Current Feature: ' ,feat)\n",
    "\n",
    "        t1 = time.time()\n",
    "        df_vimp        = deepcopy(df_train_pd)\n",
    "        df_vimp[feat]  =  np.nan\n",
    "\n",
    "        t2 = time.time()\n",
    "        pred_varimp    = xmod.predict(xgb.DMatrix(df_vimp))  \n",
    "        t3             = time.time()\n",
    "        \n",
    "        print('time to predict: ', t3-t2)\n",
    "        \n",
    "        print('\\n')\n",
    "        tmp_array.append(pred_varimp)\n",
    "\n",
    "        #Create variable importance Dataframe:\n",
    "    var_imp_val             = list(map(list, zip(*tmp_array)))\n",
    "    df_var_imp_no_index     = pd.DataFrame(var_imp_val, columns = features)\n",
    "    df_var_imp              = pd.concat([index_1,df_var_imp_no_index], axis=1)\n",
    "    df_var_imp              = df_var_imp.set_index(index_1.columns.tolist())\n",
    "\n",
    "    #Create difference dataframe with (p_all - p_i) values:\n",
    "    df_vi_diff_no_index     = abs(df_var_imp_no_index.sub(s_p_all,axis=0))\n",
    "    df_vi_diff              = pd.concat([index_1,df_vi_diff_no_index], axis=1)\n",
    "    df_vi_diff              = df_vi_diff.set_index(index_1.columns.tolist())\n",
    "    \n",
    "    return df_var_imp, df_vi_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_var_imp, df_vi_diff = var_importance(df_train_pd,index_df[:cp],'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorter(row):\n",
    "    row = row.sort_values(ascending=False)\n",
    "    return list(zip(row.index, row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = deepcopy(df_vi_diff)\n",
    "top_vals = a.apply(lambda row: sorter(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading New Data to MapD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe for Predicted values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predcol= response+'_pred'\n",
    "# predtab= table+'_predictions'\n",
    "# predview= predtab+'_view'\n",
    "\n",
    "# pdf_pred = df_pred.to_pandas()\n",
    "# # pdf_pred.reset_index(inplace=True, drop=True)\n",
    "# pdf_pred[predcol] = pd.DataFrame(pred_val[np.newaxis][0].T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdf_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table/view for predictions in MapD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.CAR_MODEL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query_pred_drop = 'DROP TABLE IF EXISTS {};'.format(predtab)\n",
    "# query_pred_create = 'CREATE TABLE IF NOT EXISTS {}({} BIGINT NOT NULL, {} FLOAT);'.format(\n",
    "#     predtab, 'mapid', predcol)\n",
    "# query_view_drop = 'DROP VIEW IF EXISTS {};'.format(predview)\n",
    "# query_view = \"CREATE VIEW {} AS (select a.*,b.{} from {} a LEFT JOIN {} b ON a.{} = b.{});\".format(\n",
    "#     predview, predcol, table, predtab, 'rowid', 'mapid')\n",
    "\n",
    "# cur = con.cursor()\n",
    "# cur.execute(query_pred_drop)\n",
    "# cur.execute(query_pred_create)\n",
    "# cur.execute(query_view_drop)\n",
    "# # cur.execute('drop view if exists churn_predictions_view')\n",
    "# # cur.execute('drop table if exists churn_predictions')\n",
    "# cur.execute(query_view)\n",
    "\n",
    "# cur.close()   # close the cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lst= time.time()\n",
    "# con.load_table(predtab,pdf_pred.itertuples(index=False))\n",
    "# let= time.time()\n",
    "\n",
    "# print('Total time taken to load the records {}'.format(let-lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con.get_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
