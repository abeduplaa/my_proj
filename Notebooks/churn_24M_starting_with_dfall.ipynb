{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Dataset : PyMapD- XGBoost - MapD\n",
    "# Response Variable: Evasion_24M\n",
    "# Using Pre-processing --> going as far as possible with PyMapD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymapd\n",
    "import pygdf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "le= LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up MapD connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection(mapd://mapd:***@http://localhost:9090/mapd?protocol=http)\n"
     ]
    }
   ],
   "source": [
    "dbname    = 'mapd'\n",
    "username  = 'mapd'\n",
    "password  = 'HyperInteractive'\n",
    "hostname  = 'localhost'\n",
    "port      = 9090\n",
    "\n",
    "con = pymapd.connect(user=username,password=password,dbname=dbname,host=hostname,port=port,protocol='http')\n",
    "print(con)\n",
    "c   = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from MapD to PyGDF\n",
    "\n",
    "#### Create Table evasion_v2_cm, and load data into table\n",
    "\n",
    "** 2 Tables will be created. Evasion_v2_cm is more processed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!cd /tmp/my_docker/base/scripts/ && ls && ./create_evasion_table.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 49\n"
     ]
    }
   ],
   "source": [
    "table= 'evasion_v2'\n",
    "response= 'EVASION_FLAG_24M'\n",
    "\n",
    "date_column = '''REFERENCE_DATE'''\n",
    "\n",
    "columns= '''PRIVATE_CUSTOMER ,TRAIN_TEST ,NUMBER_OF_CAMPAIGNS_RECEIVED ,MARKETING_PERMISSION ,TELEPHONE_AND_MAIL_PERMISSION ,DURATION_OF_OWNERSHIP ,NUMBER_OF_CARS_OWNED_BEFORE ,CAR_AGE ,CAR_BOUGHT_AT_VW_DEALER ,CAR_MODEL ,CAR_PRICE ,CO2_EMISSIONS ,PRODUCTION_YEAR ,EXTENDED_WARRANTY ,SERVICE_AND_MAINTEN_PACKAGE ,WARRANTY_LEFT ,ECONOMY_PARTS_12M ,MAINTENANCE_COSTS ,MAINTENANCE_COSTS_12M ,NUM_MAINTENANCE ,NUM_MAINTENANCE_12M , NUM_REPAIRS ,NUM_REPAIRS_12M ,NUM_WARRANTY ,REPAIR_COSTS ,REPAIR_COSTS_12M ,SERVICE_COSTS ,SERVICE_COSTS_12M ,TOTAL_COSTS ,WARRANTY_COSTS ,WARRANTY_COSTS_12M ,AVG_DURATION ,MILEAGE ,NEXT_MOT ,NUM_WORKSHOP_VISITS ,NUM_WORKSHOP_VISITS_12M ,SHARE_REPAIR_CASES ,SHARE_REPAIR_CASES_12M ,VIN_HASHED ,CUSTOMER_ID_HASHED ,MODEL_CODE,ENGINE_POWER ,ENGINE_POWER_KW_0  ,ENGINE_POWER_KW_1 ,ENGINE_POWER_COL1_0 ,HORSE_POWER  ,HORSE_POWER_0  ,HORSE_POWER_1'''\n",
    "columns_str= '''CAR_MODEL,PRODUCTION_YEAR'''\n",
    "\n",
    "\n",
    "print('Number of Columns: %d'%(len((columns+','+response).split(','))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRIVATE_CUSTOMER ,TRAIN_TEST ,NUMBER_OF_CAMPAIGNS_RECEIVED ,MARKETING_PERMISSION ,TELEPHONE_AND_MAIL_PERMISSION ,DURATION_OF_OWNERSHIP ,NUMBER_OF_CARS_OWNED_BEFORE ,CAR_AGE ,CAR_BOUGHT_AT_VW_DEALER ,CAR_MODEL ,CAR_PRICE ,CO2_EMISSIONS ,PRODUCTION_YEAR ,EXTENDED_WARRANTY ,SERVICE_AND_MAINTEN_PACKAGE ,WARRANTY_LEFT ,ECONOMY_PARTS_12M ,MAINTENANCE_COSTS ,MAINTENANCE_COSTS_12M ,NUM_MAINTENANCE ,NUM_MAINTENANCE_12M , NUM_REPAIRS ,NUM_REPAIRS_12M ,NUM_WARRANTY ,REPAIR_COSTS ,REPAIR_COSTS_12M ,SERVICE_COSTS ,SERVICE_COSTS_12M ,TOTAL_COSTS ,WARRANTY_COSTS ,WARRANTY_COSTS_12M ,AVG_DURATION ,MILEAGE ,NEXT_MOT ,NUM_WORKSHOP_VISITS ,NUM_WORKSHOP_VISITS_12M ,SHARE_REPAIR_CASES ,SHARE_REPAIR_CASES_12M ,VIN_HASHED ,CUSTOMER_ID_HASHED ,MODEL_CODE,ENGINE_POWER ,ENGINE_POWER_KW_0  ,ENGINE_POWER_KW_1 ,ENGINE_POWER_COL1_0 ,HORSE_POWER  ,HORSE_POWER_0  ,HORSE_POWER_1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Pre-Processing (DF_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unwanted columns/ Remove rowid \n",
    "\n",
    "\n",
    "### comments:\n",
    "\n",
    "**THIS PART IS DONE ON ALL THE DATA (HENCE THE DF_ALL DATAFRAME)**\n",
    "\n",
    "### TO-DO:\n",
    "- Analyze the data on the gpu (using the pygdf), and make on the fly corrections here on the jupyter notebook.\n",
    "- From the cleaned data, save the data to df_train\n",
    "\n",
    "**Remove these columns, (From Asghar's r file):**\n",
    "- reference_date (already removed)\n",
    "- private_customer\n",
    "- evasion flags\n",
    "- train_test\n",
    "- customer_id_hashed\n",
    "- vin_hashed\n",
    "- engine_power\n",
    "- engine power_kW_0\n",
    "- horse power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query all the data from the selected rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  2196754\n"
     ]
    }
   ],
   "source": [
    "#create a df for all the data:\n",
    "query_all = '''Select {},{} from {}'''.format(response,columns,table)\n",
    "df_all    = con.select_ipc_gpu(query_all,device_id=0)\n",
    "print('number of rows: ', len(df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_all is now a pygdf dataframe. We will *hopefully* use it the whole time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the unnecessary columns:\n",
    "rm_cols = set(['PRIVATE_CUSTOMER','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "#vin_hash_series = df_all['VIN_HASHED'].to_pandas()\n",
    "\n",
    "for col in rm_cols:\n",
    "    df_all.drop_column(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('float64'), dtype('float32'), category, dtype('int32')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{df_all[k].dtype for k in df_all.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EVASION_FLAG_24M                    int32\n",
       "TRAIN_TEST                       category\n",
       "NUMBER_OF_CAMPAIGNS_RECEIVED        int32\n",
       "MARKETING_PERMISSION                int32\n",
       "TELEPHONE_AND_MAIL_PERMISSION       int32\n",
       "DURATION_OF_OWNERSHIP               int32\n",
       "NUMBER_OF_CARS_OWNED_BEFORE         int32\n",
       "CAR_AGE                           float32\n",
       "CAR_BOUGHT_AT_VW_DEALER             int32\n",
       "CAR_MODEL                        category\n",
       "CAR_PRICE                         float32\n",
       "CO2_EMISSIONS                     float32\n",
       "PRODUCTION_YEAR                  category\n",
       "EXTENDED_WARRANTY                   int32\n",
       "SERVICE_AND_MAINTEN_PACKAGE       float32\n",
       "WARRANTY_LEFT                     float32\n",
       "ECONOMY_PARTS_12M                 float32\n",
       "MAINTENANCE_COSTS                 float32\n",
       "MAINTENANCE_COSTS_12M             float32\n",
       "NUM_MAINTENANCE                     int32\n",
       "NUM_MAINTENANCE_12M                 int32\n",
       "NUM_REPAIRS                         int32\n",
       "NUM_REPAIRS_12M                     int32\n",
       "NUM_WARRANTY                        int32\n",
       "REPAIR_COSTS                      float32\n",
       "REPAIR_COSTS_12M                  float32\n",
       "SERVICE_COSTS                     float32\n",
       "SERVICE_COSTS_12M                 float32\n",
       "TOTAL_COSTS                       float32\n",
       "WARRANTY_COSTS                    float32\n",
       "WARRANTY_COSTS_12M                float32\n",
       "AVG_DURATION                      float32\n",
       "MILEAGE                             int32\n",
       "NEXT_MOT                          float32\n",
       "NUM_WORKSHOP_VISITS                 int32\n",
       "NUM_WORKSHOP_VISITS_12M             int32\n",
       "SHARE_REPAIR_CASES                  int32\n",
       "SHARE_REPAIR_CASES_12M              int32\n",
       "VIN_HASHED                          int32\n",
       "CUSTOMER_ID_HASHED                  int32\n",
       "ENGINE_POWER_KW_1                   int32\n",
       "ENGINE_POWER_COL1_0               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cols = set()\n",
    "cat_cols = set()\n",
    "response_set = set([response])\n",
    "#train_test   = set(['TRAIN_TEST'])\n",
    "feature_names = set(df_all.columns) - response_set #- train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_type = df_all['CAR_MODEL'].dtype\n",
    "category_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniques = {}\n",
    "k = 5\n",
    "for i in feature_names:\n",
    "    if df_all[i].dtype is category_type:\n",
    "        cat_cols.add(i)\n",
    "    else:\n",
    "        try:\n",
    "            uniquevals = df_all[i].unique_k(k=k)\n",
    "            uniques[i] = uniquevals\n",
    "        except ValueError:\n",
    "            # more than 5 unique values\n",
    "            num_cols.add(i)\n",
    "        else:\n",
    "            # within 5 unique values\n",
    "            nunique = len(uniquevals)\n",
    "            if 1 < nunique < k:\n",
    "                cat_cols.add(i)  # as cat column\n",
    "                #print(1)\n",
    "            else:\n",
    "                num_cols.add(i)  # as num column\n",
    "                #print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECONOMY_PARTS_12M float32\n",
      "TELEPHONE_AND_MAIL_PERMISSION int32\n",
      "PRODUCTION_YEAR category\n",
      "TRAIN_TEST category\n",
      "MARKETING_PERMISSION int32\n",
      "CAR_MODEL category\n",
      "CAR_BOUGHT_AT_VW_DEALER int32\n",
      "EXTENDED_WARRANTY int32\n",
      "SHARE_REPAIR_CASES_12M int32\n",
      "SERVICE_AND_MAINTEN_PACKAGE float32\n",
      "SHARE_REPAIR_CASES int32\n"
     ]
    }
   ],
   "source": [
    "for i in cat_cols:\n",
    "    print(i,df_all[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR_AGE float32\n",
      "NUMBER_OF_CAMPAIGNS_RECEIVED int32\n",
      "SERVICE_COSTS float32\n",
      "NUM_MAINTENANCE int32\n",
      "NEXT_MOT float32\n",
      "NUM_WORKSHOP_VISITS int32\n",
      "ENGINE_POWER_COL1_0 float64\n",
      "MILEAGE int32\n",
      "CAR_PRICE float32\n",
      "NUM_REPAIRS int32\n",
      "NUMBER_OF_CARS_OWNED_BEFORE int32\n",
      "NUM_MAINTENANCE_12M int32\n",
      "WARRANTY_COSTS_12M float32\n",
      "MAINTENANCE_COSTS float32\n",
      "REPAIR_COSTS float32\n",
      "WARRANTY_COSTS float32\n",
      "DURATION_OF_OWNERSHIP int32\n",
      "NUM_REPAIRS_12M int32\n",
      "SERVICE_COSTS_12M float32\n",
      "ENGINE_POWER_KW_1 int32\n",
      "NUM_WARRANTY int32\n",
      "CUSTOMER_ID_HASHED int32\n",
      "TOTAL_COSTS float32\n",
      "NUM_WORKSHOP_VISITS_12M int32\n",
      "CO2_EMISSIONS float32\n",
      "REPAIR_COSTS_12M float32\n",
      "MAINTENANCE_COSTS_12M float32\n",
      "VIN_HASHED int32\n",
      "WARRANTY_LEFT float32\n",
      "AVG_DURATION float32\n"
     ]
    }
   ],
   "source": [
    "for i in num_cols:\n",
    "    print(i,df_all[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop near constant NUMBER_OF_CAMPAIGNS_RECEIVED\n"
     ]
    }
   ],
   "source": [
    "#FILL NA WITH MEAN:\n",
    "for i in num_cols:\n",
    "    i\n",
    "    df_all[i] = df_all[i].fillna(df_all[i].mean())\n",
    "    assert df_all[i].null_count == 0\n",
    "    std = df_all[i].std()\n",
    "    # drop near constant columns\n",
    "    if not np.isfinite(std) or std < 1e-4:\n",
    "        del df_all[i]\n",
    "        print('drop near constant', i)\n",
    "    #else:\n",
    "    #    df_all[i] = df_all[i].scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand categorical columns\n",
    "* Was not able to use label_encoder or ohe on category datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECONOMY_PARTS_12M\n",
      "TELEPHONE_AND_MAIL_PERMISSION\n",
      "PRODUCTION_YEAR\n",
      "TRAIN_TEST\n",
      "MARKETING_PERMISSION\n",
      "CAR_MODEL\n",
      "CAR_BOUGHT_AT_VW_DEALER\n",
      "EXTENDED_WARRANTY\n",
      "SHARE_REPAIR_CASES_12M\n",
      "SERVICE_AND_MAINTEN_PACKAGE\n",
      "SHARE_REPAIR_CASES\n"
     ]
    }
   ],
   "source": [
    "for i in cat_cols:\n",
    "    d_temp= df_all[i].fillna(-1).to_pandas()\n",
    "    fit= le.fit(d_temp.astype(str))\n",
    "    df_all[i] = fit.transform(d_temp.astype(str))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196754\n"
     ]
    }
   ],
   "source": [
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVASION_FLAG_24M</th>\n",
       "      <th>TRAIN_TEST</th>\n",
       "      <th>MARKETING_PERMISSION</th>\n",
       "      <th>TELEPHONE_AND_MAIL_PERMISSION</th>\n",
       "      <th>DURATION_OF_OWNERSHIP</th>\n",
       "      <th>NUMBER_OF_CARS_OWNED_BEFORE</th>\n",
       "      <th>CAR_AGE</th>\n",
       "      <th>CAR_BOUGHT_AT_VW_DEALER</th>\n",
       "      <th>CAR_MODEL</th>\n",
       "      <th>CAR_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>MILEAGE</th>\n",
       "      <th>NEXT_MOT</th>\n",
       "      <th>NUM_WORKSHOP_VISITS</th>\n",
       "      <th>NUM_WORKSHOP_VISITS_12M</th>\n",
       "      <th>SHARE_REPAIR_CASES</th>\n",
       "      <th>SHARE_REPAIR_CASES_12M</th>\n",
       "      <th>VIN_HASHED</th>\n",
       "      <th>CUSTOMER_ID_HASHED</th>\n",
       "      <th>ENGINE_POWER_KW_1</th>\n",
       "      <th>ENGINE_POWER_COL1_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>492</td>\n",
       "      <td>1915</td>\n",
       "      <td>5.50137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33713.101562</td>\n",
       "      <td>...</td>\n",
       "      <td>79255</td>\n",
       "      <td>1603.581909</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113314</td>\n",
       "      <td>95658</td>\n",
       "      <td>71</td>\n",
       "      <td>1.443759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>1467</td>\n",
       "      <td>1.49863</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>63573.199219</td>\n",
       "      <td>...</td>\n",
       "      <td>30256</td>\n",
       "      <td>-48.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100225</td>\n",
       "      <td>192896</td>\n",
       "      <td>81</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>1300</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>56600.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-49.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>355485</td>\n",
       "      <td>105876</td>\n",
       "      <td>92</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>492</td>\n",
       "      <td>1915</td>\n",
       "      <td>4.00274</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>45901.601562</td>\n",
       "      <td>...</td>\n",
       "      <td>21396</td>\n",
       "      <td>1603.581909</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120728</td>\n",
       "      <td>95658</td>\n",
       "      <td>71</td>\n",
       "      <td>1.443759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>602</td>\n",
       "      <td>26</td>\n",
       "      <td>4.49863</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>39672.101562</td>\n",
       "      <td>...</td>\n",
       "      <td>13845</td>\n",
       "      <td>-608.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11668</td>\n",
       "      <td>3896</td>\n",
       "      <td>55</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EVASION_FLAG_24M  TRAIN_TEST  MARKETING_PERMISSION  \\\n",
       "0                 1           0                     0   \n",
       "1                 0           2                     1   \n",
       "2                 1           1                     2   \n",
       "3                 1           0                     0   \n",
       "4                 0           2                     1   \n",
       "\n",
       "   TELEPHONE_AND_MAIL_PERMISSION  DURATION_OF_OWNERSHIP  \\\n",
       "0                              0                    492   \n",
       "1                              1                    314   \n",
       "2                              2                     47   \n",
       "3                              0                    492   \n",
       "4                              1                    602   \n",
       "\n",
       "   NUMBER_OF_CARS_OWNED_BEFORE  CAR_AGE  CAR_BOUGHT_AT_VW_DEALER  CAR_MODEL  \\\n",
       "0                         1915  5.50137                        1          1   \n",
       "1                         1467  1.49863                        1          5   \n",
       "2                         1300  1.00000                        1         10   \n",
       "3                         1915  4.00274                        1          5   \n",
       "4                           26  4.49863                        1          2   \n",
       "\n",
       "      CAR_PRICE         ...           MILEAGE     NEXT_MOT  \\\n",
       "0  33713.101562         ...             79255  1603.581909   \n",
       "1  63573.199219         ...             30256   -48.000000   \n",
       "2  56600.000000         ...                11   -49.000000   \n",
       "3  45901.601562         ...             21396  1603.581909   \n",
       "4  39672.101562         ...             13845  -608.000000   \n",
       "\n",
       "   NUM_WORKSHOP_VISITS  NUM_WORKSHOP_VISITS_12M  SHARE_REPAIR_CASES  \\\n",
       "0                    8                        0                   0   \n",
       "1                    2                        2                   0   \n",
       "2                    1                        1                   0   \n",
       "3                    4                        3                   0   \n",
       "4                    5                        0                   0   \n",
       "\n",
       "   SHARE_REPAIR_CASES_12M  VIN_HASHED  CUSTOMER_ID_HASHED  ENGINE_POWER_KW_1  \\\n",
       "0                       0      113314               95658                 71   \n",
       "1                       0      100225              192896                 81   \n",
       "2                       0      355485              105876                 92   \n",
       "3                       0      120728               95658                 71   \n",
       "4                       0       11668                3896                 55   \n",
       "\n",
       "   ENGINE_POWER_COL1_0  \n",
       "0             1.443759  \n",
       "1             2.000000  \n",
       "2             1.400000  \n",
       "3             1.443759  \n",
       "4             1.600000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DF_Train and DF_Test\n",
    "\n",
    "**As of now, this part is created from the original data. It is not based on the data pre-processing done above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'pygdf.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-908c659076f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRAIN_TEST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vwdatalab/Desktop/repos/pygdf/pygdf/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mcol_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <class 'pygdf.series.Series'>"
     ]
    }
   ],
   "source": [
    "df_train = df_all.loc[df_all['TRAIN_TEST']== 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','VIN_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "#rm_cols = set(['PRIVATE_CUSTOMER','TRAIN_TEST','CUSTOMER_ID_HASHED','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n",
    "\n",
    "vin_hash_series = df_train['VIN_HASHED'].to_pandas()\n",
    "\n",
    "for col in rm_cols:\n",
    "    df_train.drop_column(col)\n",
    "    df_test.drop_column(col)\n",
    "    #df_all.drop_column(col)\n",
    "    \n",
    "df_pred=pygdf.DataFrame()\n",
    "df_pred.add_column('mapid', df_test['mapid'])\n",
    "df_test.drop_column('mapid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cat cols : Labels Encoding\n",
    "#### Comments:\n",
    "\n",
    "fillna, what does it do? does this mask null with the value -1? how could this affect the ML process?\n",
    "\n",
    "<span style=\"color:green\"> **the -1 is used for label encoding. this is only for string columns** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in columns_str.split(','):\n",
    "    ctrain= df_train[col].fillna(-1).to_pandas()\n",
    "    ctest= df_test[col].fillna(-1).to_pandas()\n",
    "    fit= le.fit(ctrain.astype(str))\n",
    "    df_train[col] = fit.transform(ctrain.astype(str))\n",
    "    print(col)\n",
    "    df_test[col] = fit.transform(ctest.astype(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fill na/nan\n",
    "\n",
    "### Comments:\n",
    "* Why are na values being filled with -999? How does the model handle -999 values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.head(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.head(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into 80:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- Split should be done differently\n",
    "- Testing should be on data that the df has never seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_num = 0.8\n",
    "cp = int(len(df_train)*split_num)\n",
    "df_train_n, df_val = df_train.loc[:cp], df_train.loc[cp:]\n",
    "\n",
    "print(len(df_train_n))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GPU DF/matrices of Training;Val;Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_y= df_train_n[response]\n",
    "df_val_y= df_val[response]\n",
    "df_test_y= df_test[response]\n",
    "\n",
    "del df_train_n[response]\n",
    "del df_val[response]\n",
    "del df_test[response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters\n",
    "\n",
    "### Comments:\n",
    "<span style=\"color:red\"> **We need to tune the parameters here. I'm not sure if these parameters are optimal. We should look in to how to optimize parameters** </span>\n",
    "\n",
    "* How was the performance metric done in the UC previously?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params= {}\n",
    "params['objective']= 'binary:logistic'\n",
    "params['eval_metric']= 'auc'\n",
    "params['max_depth']= 7\n",
    "params['eta']= 0.3\n",
    "params['silent']= 0\n",
    "#params['tree_method']= 'gpu_exact'\n",
    "params['tree_method']= 'gpu_hist'\n",
    "\n",
    "num_round= 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from matrices\n",
    "check to make sure before training that the matrices look normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_n.head(3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_y.head(3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_pd = df_train_n.to_pandas()\n",
    "df_val_pd = df_val.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cpst= time.time()\n",
    "dtrain= xgb.DMatrix(df_train_pd,label=df_train_y.to_pandas())\n",
    "cpet= time.time()\n",
    "print('Time taken for Copying Data: {}'.format(cpet-cpst))\n",
    "\n",
    "st= time.time()\n",
    "xmod= xgb.train(params,dtrain,num_round)\n",
    "en= time.time()\n",
    "print('Time taken for training: {}'.format(en-st))\n",
    "\n",
    "err_val= xmod.eval(xgb.DMatrix(df_val_pd,label=df_val_y.to_pandas()))\n",
    "print('Validation Accuracy: {}'.format(err_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize']= [15,12]\n",
    "matplotlib.rcParams['figure.dpi']= 55\n",
    "plot_importance(xmod)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spt= time.time()\n",
    "pred_val= xmod.predict(xgb.DMatrix(df_test.to_pandas()))\n",
    "ept= time.time()\n",
    "err_pred= xmod.eval(xgb.DMatrix(df_test.to_pandas(),df_test_y.to_pandas()))\n",
    "\n",
    "print('Time taken for Predictions: {}'.format(ept-spt))\n",
    "print('Predictions Accuracy: {}'.format(err_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top features for grid creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var= 10  # Number of variables to show\n",
    "varimp= xmod.get_fscore()\n",
    "varimp= sorted(varimp.items(),key=lambda val: val[1],reverse=True)\n",
    "pdf_varimp= pd.DataFrame(varimp).iloc[0:var,:]\n",
    "pdf_varimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridCols = pdf_varimp[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create Partial Dependency grid\n",
    "\n",
    "- Potential issue with -999 as null values in grid creation. may have to switch them back to Null or another value?\n",
    "- also, should there be a grid for each feature? \n",
    "\n",
    "\n",
    "### Calculate Partial Dependency\n",
    "\n",
    "1. the min and max values of the feature are saved to f_min and f_max\n",
    "2. xi is the array of values that are being used for calculation of the Partial Dependency Analysis\n",
    "\n",
    "####  Potential Problems:\n",
    "\n",
    "- Using -999 as null values. Create\n",
    "- how to store all the values?\n",
    "- What would be the total amount of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resolution = 50     # the resolution for the Partial Dependence analysis for non-categorical columns\n",
    "array1 = []# resolution array\n",
    "print('# of rows in dataframe: ', resolution*var)\n",
    "\n",
    "cat_col = ['MARKETING_PERMISSION','TELEPHONE_AND_MAIL_PERMISSION','CAR_BOUGHT_AT_VW_DEALER','CAR_MODEL','EXTENDED_WARRANTY']\n",
    "\n",
    "#for gpu prediction use 1. for cpu, use 0\n",
    "gpu_or_cpu = 0\n",
    "\n",
    "if gpu_or_cpu == 1:\n",
    "    xmod.set_param({\"predictor\":\"gpu_predictor\"})\n",
    "else:\n",
    "    xmod.set_param({\"predictor\":\"cpu_predictor\"})\n",
    "    \n",
    "    \n",
    "for feat in gridCols:\n",
    "    print('Calculating for feature: ', feat)\n",
    "    \n",
    "    #Take the smallest and largest feature value\n",
    "    f_min = df_train_pd[feat].min()\n",
    "    f_max = df_train_pd[feat].max()\n",
    "    \n",
    "    #Make a linearly increasing array for xi. If categorical, use the values:\n",
    "    if feat in cat_col:\n",
    "        xi = df_train_pd[feat].unique()\n",
    "    else:\n",
    "        xi = [f_min + x*(f_max-f_min)/resolution for x in range(resolution)]\n",
    "    \n",
    "    #save df_train_pd as grid\n",
    "    grid = deepcopy(df_train_pd)\n",
    "    \n",
    "    #calculate partial dependency for each value of i\n",
    "    for i in xi:\n",
    "        grid[feat] = i\n",
    "        t1 = time.time()\n",
    "        pred_p1 = xmod.predict(xgb.DMatrix(grid))\n",
    "        tf = time.time()-t1\n",
    "        print('time to predict: ', tf)\n",
    "        p_d = ( ( 1 / grid.shape[0] ) * sum(pred_p1)) #calculation of partial dependency\n",
    "        array1.append([feat, i , p_d])\n",
    "\n",
    "#save in to dataframe:\n",
    "par_dep_df = pd.DataFrame(array1, columns=['feature', 'x', 'p_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par_dep_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Partial Dependence Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get unique values of features:\n",
    "features_pardep = par_dep_df['feature'].unique()\n",
    "for feat in features_pardep:\n",
    "    plt.figure()    \n",
    "    df_tmp = par_dep_df[par_dep_df['feature']==feat]\n",
    "    df_tmp.plot(x='x', y='p_d')\n",
    "    plt.title(feat)\n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel('Partial Dependence')\n",
    "    #plt.set_ylim(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train_pd.shape[0]\n",
    "df_train_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: \n",
    "\n",
    " Predict on whole data set using model, xmod. This value is p_all\n",
    "\n",
    "Pre step: Calculate p_all\n",
    "\n",
    "1. Set a feature to NULL (essentially remove it from the model)\n",
    "2. Predict the values for the data set with one feature null p_i\n",
    "3. calculate difference between p_all and p_i. This will be a measure of importance p_importance = p_i - p_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define features:\n",
    "features = df_train_pd.columns\n",
    "\n",
    "#temporary array used to store the data, to then be made to pandas.\n",
    "tmp_array = []\n",
    "\n",
    "#for gpu prediction use 1. for cpu, use 0\n",
    "gpu_or_cpu = 0\n",
    "\n",
    "if gpu_or_cpu == 1:\n",
    "    xmod.set_param({\"predictor\":\"gpu_predictor\"})\n",
    "else:\n",
    "    xmod.set_param({\"predictor\":\"cpu_predictor\"})\n",
    "\n",
    "#calculate p_all:\n",
    "p_all = xmod.predict( xgb.DMatrix(df_train_pd) )\n",
    "s_p_all = pd.Series(p_all)\n",
    "\n",
    "#calculate variable importance:\n",
    "for feat in features:\n",
    "    print('\\n Current Feature: ' ,feat)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    df_vimp        = deepcopy(df_train_pd)\n",
    "    df_vimp[feat]  =  np.nan\n",
    "    df_vimp_xgb    = xgb.DMatrix(df_vimp)\n",
    "    t2 = time.time()\n",
    "    print('time to create xgb matrix: ' ,t2-t1)\n",
    "    \n",
    "    pred_varimp    = xmod.predict(df_vimp_xgb)  \n",
    "    t3             = time.time()\n",
    "    print('time to predict: ', t3-t2)\n",
    "    \n",
    "    tmp_array.append(pred_varimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes\n",
    "- Create dataframe for variable importance matrix\n",
    "- Create dataframe for difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create variable importance Dataframe:\n",
    "var_imp_val               = list(map(list, zip(*tmp_array)))\n",
    "df_var_imp_no_index       = pd.DataFrame(var_imp_val, columns = features)\n",
    "df_var_imp                = pd.concat([vin_hash_series.loc[:cp],df_var_imp_no_index], axis=1)\n",
    "df_var_imp                = df_var_imp.rename(index=str, columns={0: \"VIN_HASHED\"})\n",
    "df_var_imp['VIN_HASHED']  = df_var_imp['VIN_HASHED'].astype(int)\n",
    "df_var_imp = df_var_imp.set_index('VIN_HASHED')\n",
    "\n",
    "#Create difference dataframe with (p_all - p_i) values:\n",
    "df_vi_diff_no_index       = abs(df_var_imp_no_index.sub(s_p_all,axis=0))\n",
    "df_vi_diff                = pd.concat([vin_hash_series.loc[:cp],df_vi_diff_no_index], axis=1)\n",
    "df_vi_diff                = df_vi_diff.rename(index=str, columns={0: \"VIN_HASHED\"})\n",
    "df_vi_diff['VIN_HASHED']  = df_vi_diff['VIN_HASHED'].astype(int)\n",
    "df_vi_diff = df_vi_diff.set_index('VIN_HASHED')\n",
    "\n",
    "#df_vi_diff       = abs(df_var_imp.sub(s_p_all,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Problem: There are duplicate VIN_hash values\n",
    "df_var_imp[df_var_imp.index.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data in MapD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe for Predicted values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predcol= response+'_pred'\n",
    "# predtab= table+'_predictions'\n",
    "# predview= predtab+'_view'\n",
    "\n",
    "# pdf_pred = df_pred.to_pandas()\n",
    "# # pdf_pred.reset_index(inplace=True, drop=True)\n",
    "# pdf_pred[predcol] = pd.DataFrame(pred_val[np.newaxis][0].T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdf_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table/view for predictions in MapD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query_pred_drop = 'DROP TABLE IF EXISTS {};'.format(predtab)\n",
    "# query_pred_create = 'CREATE TABLE IF NOT EXISTS {}({} BIGINT NOT NULL, {} FLOAT);'.format(\n",
    "#     predtab, 'mapid', predcol)\n",
    "# query_view_drop = 'DROP VIEW IF EXISTS {};'.format(predview)\n",
    "# query_view = \"CREATE VIEW {} AS (select a.*,b.{} from {} a LEFT JOIN {} b ON a.{} = b.{});\".format(\n",
    "#     predview, predcol, table, predtab, 'rowid', 'mapid')\n",
    "\n",
    "# cur = con.cursor()\n",
    "# cur.execute(query_pred_drop)\n",
    "# cur.execute(query_pred_create)\n",
    "# cur.execute(query_view_drop)\n",
    "# # cur.execute('drop view if exists churn_predictions_view')\n",
    "# # cur.execute('drop table if exists churn_predictions')\n",
    "# cur.execute(query_view)\n",
    "\n",
    "# cur.close()   # close the cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lst= time.time()\n",
    "# con.load_table(predtab,pdf_pred.itertuples(index=False))\n",
    "# let= time.time()\n",
    "\n",
    "# print('Total time taken to load the records {}'.format(let-lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con.get_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
